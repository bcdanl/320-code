{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbgAcbEbafZPXTdC8LIDRc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bcdanl/320-code/blob/main/danl_320_script_2025_0212.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Dlwr1NOP2bUG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df_pd = pd.read_csv('https://bcdanl.github.io/data/nba.csv')\n",
        "df = spark.createDataFrame(df_pd)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ickT7TI_3Z3x",
        "outputId": "f901c938-f920-4558-99bf-b2c672ea8884"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Birthday: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3ma-ADM31U8",
        "outputId": "1a62fb50-6921-4b82-d574-bc6c689a0779"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------+--------+--------+\n",
            "|           Name|                Team|Position|Birthday|  Salary|\n",
            "+---------------+--------------------+--------+--------+--------+\n",
            "|   Shake Milton|  Philadelphia 76ers|      SG| 9/26/96| 1445697|\n",
            "| Christian Wood|     Detroit Pistons|      PF| 9/27/95| 1645357|\n",
            "|  PJ Washington|   Charlotte Hornets|      PF| 8/23/98| 3831840|\n",
            "|   Derrick Rose|     Detroit Pistons|      PG| 10/4/88| 7317074|\n",
            "|  Marial Shayok|  Philadelphia 76ers|       G| 7/26/95|   79568|\n",
            "| Draymond Green|Golden State Warr...|      PF|  3/4/90|18539130|\n",
            "|  Kendrick Nunn|          Miami Heat|      SG|  8/3/95| 1416852|\n",
            "|     Cedi Osman| Cleveland Cavaliers|      SF|  4/8/95| 2907143|\n",
            "|    Brook Lopez|     Milwaukee Bucks|       C|  4/1/88|12093024|\n",
            "|   Torrey Craig|      Denver Nuggets|      SF|12/19/90| 2000000|\n",
            "|Jordan Clarkson| Cleveland Cavaliers|      PG|  6/7/92|13437500|\n",
            "|    Alex Caruso|  Los Angeles Lakers|      PG| 2/28/94| 2750000|\n",
            "|   Norvel Pelle|  Philadelphia 76ers|      FC|  2/3/93|   79568|\n",
            "|  Tyler Johnson|        Phoenix Suns|      PG|  5/7/92|19245370|\n",
            "|     Alec Burks|Golden State Warr...|      SG| 7/20/91| 2320044|\n",
            "| JaMychal Green|Los Angeles Clippers|      PF| 6/21/90| 4767000|\n",
            "|  Dwight Howard|  Los Angeles Lakers|       C| 12/8/85| 5603850|\n",
            "|   Nikola Jokic|      Denver Nuggets|       C| 2/19/95|27504630|\n",
            "|  Chris Boucher|     Toronto Raptors|      PF| 1/11/93| 1588231|\n",
            "|  Marcus Morris|     New York Knicks|      PF|  9/2/89|15000000|\n",
            "+---------------+--------------------+--------+--------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cast() method"
      ],
      "metadata": {
        "id": "tF0xi5LP3m7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "df = (\n",
        "    df.withColumn(\"Salary_int\",\n",
        "                  col(\"Salary\").cast(\"int\"))\n",
        ")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FF9bSaPt3mZO",
        "outputId": "c80fd387-6ac1-4063-ec7f-2c3e9f9de3d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------+--------+--------+----------+\n",
            "|           Name|                Team|Position|Birthday|  Salary|Salary_int|\n",
            "+---------------+--------------------+--------+--------+--------+----------+\n",
            "|   Shake Milton|  Philadelphia 76ers|      SG| 9/26/96| 1445697|   1445697|\n",
            "| Christian Wood|     Detroit Pistons|      PF| 9/27/95| 1645357|   1645357|\n",
            "|  PJ Washington|   Charlotte Hornets|      PF| 8/23/98| 3831840|   3831840|\n",
            "|   Derrick Rose|     Detroit Pistons|      PG| 10/4/88| 7317074|   7317074|\n",
            "|  Marial Shayok|  Philadelphia 76ers|       G| 7/26/95|   79568|     79568|\n",
            "| Draymond Green|Golden State Warr...|      PF|  3/4/90|18539130|  18539130|\n",
            "|  Kendrick Nunn|          Miami Heat|      SG|  8/3/95| 1416852|   1416852|\n",
            "|     Cedi Osman| Cleveland Cavaliers|      SF|  4/8/95| 2907143|   2907143|\n",
            "|    Brook Lopez|     Milwaukee Bucks|       C|  4/1/88|12093024|  12093024|\n",
            "|   Torrey Craig|      Denver Nuggets|      SF|12/19/90| 2000000|   2000000|\n",
            "|Jordan Clarkson| Cleveland Cavaliers|      PG|  6/7/92|13437500|  13437500|\n",
            "|    Alex Caruso|  Los Angeles Lakers|      PG| 2/28/94| 2750000|   2750000|\n",
            "|   Norvel Pelle|  Philadelphia 76ers|      FC|  2/3/93|   79568|     79568|\n",
            "|  Tyler Johnson|        Phoenix Suns|      PG|  5/7/92|19245370|  19245370|\n",
            "|     Alec Burks|Golden State Warr...|      SG| 7/20/91| 2320044|   2320044|\n",
            "| JaMychal Green|Los Angeles Clippers|      PF| 6/21/90| 4767000|   4767000|\n",
            "|  Dwight Howard|  Los Angeles Lakers|       C| 12/8/85| 5603850|   5603850|\n",
            "|   Nikola Jokic|      Denver Nuggets|       C| 2/19/95|27504630|  27504630|\n",
            "|  Chris Boucher|     Toronto Raptors|      PF| 1/11/93| 1588231|   1588231|\n",
            "|  Marcus Morris|     New York Knicks|      PF|  9/2/89|15000000|  15000000|\n",
            "+---------------+--------------------+--------+--------+--------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N78ppLS83-hu",
        "outputId": "56c941a9-36e5-431d-9e6a-e5e0392fabd7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Birthday: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            " |-- Salary_int: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
        "\n",
        "df = df.withColumn(\"Birthday_date\",\n",
        "                   to_date(\"Birthday\", \"M/d/yy\"))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Vg5MYC84QLX",
        "outputId": "00ac4d22-d888-403d-eae6-192bb2acde2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------+--------+--------+----------+-------------+\n",
            "|           Name|                Team|Position|Birthday|  Salary|Salary_int|Birthday_date|\n",
            "+---------------+--------------------+--------+--------+--------+----------+-------------+\n",
            "|   Shake Milton|  Philadelphia 76ers|      SG| 9/26/96| 1445697|   1445697|   1996-09-26|\n",
            "| Christian Wood|     Detroit Pistons|      PF| 9/27/95| 1645357|   1645357|   1995-09-27|\n",
            "|  PJ Washington|   Charlotte Hornets|      PF| 8/23/98| 3831840|   3831840|   1998-08-23|\n",
            "|   Derrick Rose|     Detroit Pistons|      PG| 10/4/88| 7317074|   7317074|   1988-10-04|\n",
            "|  Marial Shayok|  Philadelphia 76ers|       G| 7/26/95|   79568|     79568|   1995-07-26|\n",
            "| Draymond Green|Golden State Warr...|      PF|  3/4/90|18539130|  18539130|   1990-03-04|\n",
            "|  Kendrick Nunn|          Miami Heat|      SG|  8/3/95| 1416852|   1416852|   1995-08-03|\n",
            "|     Cedi Osman| Cleveland Cavaliers|      SF|  4/8/95| 2907143|   2907143|   1995-04-08|\n",
            "|    Brook Lopez|     Milwaukee Bucks|       C|  4/1/88|12093024|  12093024|   1988-04-01|\n",
            "|   Torrey Craig|      Denver Nuggets|      SF|12/19/90| 2000000|   2000000|   1990-12-19|\n",
            "|Jordan Clarkson| Cleveland Cavaliers|      PG|  6/7/92|13437500|  13437500|   1992-06-07|\n",
            "|    Alex Caruso|  Los Angeles Lakers|      PG| 2/28/94| 2750000|   2750000|   1994-02-28|\n",
            "|   Norvel Pelle|  Philadelphia 76ers|      FC|  2/3/93|   79568|     79568|   1993-02-03|\n",
            "|  Tyler Johnson|        Phoenix Suns|      PG|  5/7/92|19245370|  19245370|   1992-05-07|\n",
            "|     Alec Burks|Golden State Warr...|      SG| 7/20/91| 2320044|   2320044|   1991-07-20|\n",
            "| JaMychal Green|Los Angeles Clippers|      PF| 6/21/90| 4767000|   4767000|   1990-06-21|\n",
            "|  Dwight Howard|  Los Angeles Lakers|       C| 12/8/85| 5603850|   5603850|   1985-12-08|\n",
            "|   Nikola Jokic|      Denver Nuggets|       C| 2/19/95|27504630|  27504630|   1995-02-19|\n",
            "|  Chris Boucher|     Toronto Raptors|      PF| 1/11/93| 1588231|   1588231|   1993-01-11|\n",
            "|  Marcus Morris|     New York Knicks|      PF|  9/2/89|15000000|  15000000|   1989-09-02|\n",
            "+---------------+--------------------+--------+--------+--------+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4F_OnuV5CVy",
        "outputId": "41479194-1ed2-4d8b-c97b-4332a864adee"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            " |-- Position: string (nullable = true)\n",
            " |-- Birthday: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            " |-- Salary_int: integer (nullable = true)\n",
            " |-- Birthday_date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 5.4"
      ],
      "metadata": {
        "id": "UlXvLouP5ctK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "nfl_pd = pd.read_csv('https://bcdanl.github.io/data/nfl.csv')\n",
        "nfl = spark.createDataFrame(nfl_pd)"
      ],
      "metadata": {
        "id": "2VijQO4C5eF-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nfl.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXz5ccVq5jte",
        "outputId": "b924f546-46e9-4a1b-852e-8aa63fb6315e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------------+--------+----------+--------+\n",
            "|            Name|                Team|Position|  Birthday|  Salary|\n",
            "+----------------+--------------------+--------+----------+--------+\n",
            "|    Tremon Smith| Philadelphia Eagles|      RB| 7/20/1996|  570000|\n",
            "|  Shawn Williams|  Cincinnati Bengals|      SS| 5/13/1991| 3500000|\n",
            "|     Adam Butler|New England Patriots|      DT| 4/12/1994|  645000|\n",
            "|     Derek Wolfe|      Denver Broncos|      DE| 2/24/1990| 8000000|\n",
            "|       Jake Ryan|Jacksonville Jaguars|     OLB| 2/27/1992| 1000000|\n",
            "| Ezekiel Elliott|      Dallas Cowboys|      RB| 7/22/1995| 3853137|\n",
            "|    John Jenkins|      Miami Dolphins|      DE| 7/11/1989|  805000|\n",
            "|  Andrew Wingard|Jacksonville Jaguars|      FS| 12/5/1996|  495000|\n",
            "|      John Jerry|  Cincinnati Bengals|      OT| 6/14/1986|  930000|\n",
            "|    Matt Barkley|       Buffalo Bills|      QB|  9/8/1990| 1250000|\n",
            "|     Josh Harris|     Atlanta Falcons|      LS| 4/27/1989|  930000|\n",
            "|  Adarius Taylor|    Cleveland Browns|     OLB| 9/21/1990| 1350000|\n",
            "| Olabisi Johnson|   Minnesota Vikings|      WR| 3/17/1997|  647500|\n",
            "|    Andrew Adams|Tampa Bay Buccaneers|      SS|10/28/1992|  720000|\n",
            "|    Forrest Lamp|Los Angeles Chargers|       G| 2/20/1994| 1071084|\n",
            "|   Ryan Kerrigan| Washington Redskins|     OLB| 8/16/1988|10500000|\n",
            "|    Robert Davis| Philadelphia Eagles|      WR|  4/2/1995|  570000|\n",
            "|     Kevin Byard|    Tennessee Titans|      FS| 8/17/1993| 2025000|\n",
            "|   Pat O'Donnell|       Chicago Bears|       P| 2/22/1991| 1250000|\n",
            "|Barkevious Mingo|      Houston Texans|     OLB| 10/4/1990| 3400000|\n",
            "+----------------+--------------------+--------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nfl = nfl.withColumn(\"Birthday\",\n",
        "                     to_date(\"Birthday\", \"M/d/yy\"))\n",
        "nfl.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB1Y87q_6Mbl",
        "outputId": "ba5ba00b-b8af-4c85-f1df-053c2a71393f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------+--------------------+--------+----------+--------+\n",
            "|            Name|                Team|Position|  Birthday|  Salary|\n",
            "+----------------+--------------------+--------+----------+--------+\n",
            "|    Tremon Smith| Philadelphia Eagles|      RB|1996-07-20|  570000|\n",
            "|  Shawn Williams|  Cincinnati Bengals|      SS|1991-05-13| 3500000|\n",
            "|     Adam Butler|New England Patriots|      DT|1994-04-12|  645000|\n",
            "|     Derek Wolfe|      Denver Broncos|      DE|1990-02-24| 8000000|\n",
            "|       Jake Ryan|Jacksonville Jaguars|     OLB|1992-02-27| 1000000|\n",
            "| Ezekiel Elliott|      Dallas Cowboys|      RB|1995-07-22| 3853137|\n",
            "|    John Jenkins|      Miami Dolphins|      DE|1989-07-11|  805000|\n",
            "|  Andrew Wingard|Jacksonville Jaguars|      FS|1996-12-05|  495000|\n",
            "|      John Jerry|  Cincinnati Bengals|      OT|1986-06-14|  930000|\n",
            "|    Matt Barkley|       Buffalo Bills|      QB|1990-09-08| 1250000|\n",
            "|     Josh Harris|     Atlanta Falcons|      LS|1989-04-27|  930000|\n",
            "|  Adarius Taylor|    Cleveland Browns|     OLB|1990-09-21| 1350000|\n",
            "| Olabisi Johnson|   Minnesota Vikings|      WR|1997-03-17|  647500|\n",
            "|    Andrew Adams|Tampa Bay Buccaneers|      SS|1992-10-28|  720000|\n",
            "|    Forrest Lamp|Los Angeles Chargers|       G|1994-02-20| 1071084|\n",
            "|   Ryan Kerrigan| Washington Redskins|     OLB|1988-08-16|10500000|\n",
            "|    Robert Davis| Philadelphia Eagles|      WR|1995-04-02|  570000|\n",
            "|     Kevin Byard|    Tennessee Titans|      FS|1993-08-17| 2025000|\n",
            "|   Pat O'Donnell|       Chicago Bears|       P|1991-02-22| 1250000|\n",
            "|Barkevious Mingo|      Houston Texans|     OLB|1990-10-04| 3400000|\n",
            "+----------------+--------------------+--------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 5.5\n",
        "Who are the five highest-paid players?\n",
        "\n"
      ],
      "metadata": {
        "id": "0oWL9C2_6hJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "\n",
        "(\n",
        "    nfl.orderBy(nfl.Salary.desc())\n",
        "    .limit(6)\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nk9vDsr36glW",
        "outputId": "e86ddfbe-a74c-4a64-a7a8-678b0243b0d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+--------------------+--------+----------+--------+\n",
            "|           Name|                Team|Position|  Birthday|  Salary|\n",
            "+---------------+--------------------+--------+----------+--------+\n",
            "|   Kirk Cousins|   Minnesota Vikings|      QB|1988-08-19|27500000|\n",
            "| Marcus Mariota|    Tennessee Titans|      QB|1993-10-30|20922000|\n",
            "| Jameis Winston|Tampa Bay Buccaneers|      QB|1994-01-06|20922000|\n",
            "|     Derek Carr|     Oakland Raiders|      QB|1991-03-28|19900000|\n",
            "|Jimmy Garoppolo| San Francisco 49Ers|      QB|1991-11-02|17200000|\n",
            "|  Melvin Ingram|Los Angeles Chargers|      DE|1989-04-26|17000000|\n",
            "+---------------+--------------------+--------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Who is the oldest player?"
      ],
      "metadata": {
        "id": "zZHo7n257Gb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(\n",
        "    nfl.orderBy(nfl.Birthday)\n",
        "    .limit(2)\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SvjFk6m7Gxq",
        "outputId": "48d9e8c8-0814-411f-a261-a7bba5cbd888"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------------+--------+----------+--------+\n",
            "|      Name|                Team|Position|  Birthday|  Salary|\n",
            "+----------+--------------------+--------+----------+--------+\n",
            "| Tom Brady|New England Patriots|      QB|1977-08-03|14000000|\n",
            "|Drew Brees|  New Orleans Saints|      QB|1979-01-15| 1400000|\n",
            "+----------+--------------------+--------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 5.6\n",
        "\n",
        "How can we sort the DataFrame first by Team in alphabetical order and then by Salary in descending order?\n",
        "\n"
      ],
      "metadata": {
        "id": "moV818O67VXo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    nfl.orderBy(\"Team\", nfl.Salary.desc())\n",
        "    .show(50)\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TS1V5q647U5E",
        "outputId": "9fedc4da-132a-400f-cc0c-5937ae3bb62f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+-----------------+--------+----------+--------+\n",
            "|              Name|             Team|Position|  Birthday|  Salary|\n",
            "+------------------+-----------------+--------+----------+--------+\n",
            "|    Chandler Jones|Arizona Cardinals|     OLB|1990-02-27|16500000|\n",
            "|  Patrick Peterson|Arizona Cardinals|      CB|1990-07-11|11000000|\n",
            "|  Larry Fitzgerald|Arizona Cardinals|      WR|1983-08-31|11000000|\n",
            "|     David Johnson|Arizona Cardinals|      RB|1991-12-16| 5700000|\n",
            "|       Justin Pugh|Arizona Cardinals|       G|1990-08-15| 5000000|\n",
            "|      Corey Peters|Arizona Cardinals|      NT|1988-06-08| 3250000|\n",
            "|      Kenyan Drake|Arizona Cardinals|      RB|1994-01-26| 2025000|\n",
            "|          Andy Lee|Arizona Cardinals|       P|1982-08-11| 2000000|\n",
            "|      Jordan Hicks|Arizona Cardinals|     ILB|1991-10-14| 2000000|\n",
            "|       Chris Jones|Arizona Cardinals|      CB|1995-08-13| 1800000|\n",
            "|      Jordan Mills|Arizona Cardinals|      OT|1990-12-24| 1700000|\n",
            "|    Haason Reddick|Arizona Cardinals|     ILB|1994-09-22| 1690270|\n",
            "|         Zach Kerr|Arizona Cardinals|      DE|1990-08-29| 1500000|\n",
            "|      Charles Clay|Arizona Cardinals|      TE|1989-02-13| 1150000|\n",
            "|       Brooks Reed|Arizona Cardinals|     OLB|1987-02-25| 1125000|\n",
            "|     Brett Hundley|Arizona Cardinals|      QB|1993-06-15| 1125000|\n",
            "|        Max Garcia|Arizona Cardinals|       G|1991-11-09| 1100000|\n",
            "|       Budda Baker|Arizona Cardinals|      FS|1996-01-10| 1085910|\n",
            "|     Rodney Gunter|Arizona Cardinals|      DE|1992-01-19| 1000000|\n",
            "|      Aaron Brewer|Arizona Cardinals|      LS|1990-07-05|  930000|\n",
            "|     Cassius Marsh|Arizona Cardinals|     OLB|1992-07-07|  850000|\n",
            "|       Chris Banjo|Arizona Cardinals|      FS|1990-02-26|  850000|\n",
            "|  Jonathan Bullard|Arizona Cardinals|      DE|1993-11-30|  820000|\n",
            "|     Maxx Williams|Arizona Cardinals|      TE|1994-04-12|  805000|\n",
            "|       Caraun Reid|Arizona Cardinals|      DE|1991-11-23|  805000|\n",
            "|    Christian Kirk|Arizona Cardinals|      WR|1996-11-18|  747948|\n",
            "|     Pharoh Cooper|Arizona Cardinals|      WR|1995-03-07|  720000|\n",
            "|      Damiere Byrd|Arizona Cardinals|      WR|1993-01-27|  720000|\n",
            "|        Joe Walker|Arizona Cardinals|     ILB|1992-12-11|  645000|\n",
            "|     Zane Gonzalez|Arizona Cardinals|       K|1995-05-07|  645000|\n",
            "|    Tanner Vallejo|Arizona Cardinals|     ILB|1994-12-16|  645000|\n",
            "|   Darrell Daniels|Arizona Cardinals|      TE|1994-11-22|  645000|\n",
            "|Charles Washington|Arizona Cardinals|      FS|1993-03-10|  645000|\n",
            "|       Kylie Fitts|Arizona Cardinals|     OLB|1994-10-11|  570000|\n",
            "|    Dennis Gardeck|Arizona Cardinals|     ILB|1994-08-09|  570000|\n",
            "|     Chase Edmonds|Arizona Cardinals|      RB|1996-04-13|  570000|\n",
            "|        Dan Arnold|Arizona Cardinals|      TE|1995-03-15|  570000|\n",
            "|     Justin Murray|Arizona Cardinals|      OT|1993-04-19|  570000|\n",
            "|        Mason Cole|Arizona Cardinals|       C|1996-03-28|  570000|\n",
            "|    Kevin Peterson|Arizona Cardinals|      CB|1994-03-22|  570000|\n",
            "|  Keishawn Bierria|Arizona Cardinals|     OLB|1995-07-26|  570000|\n",
            "|   Trent Sherfield|Arizona Cardinals|      WR|1996-02-26|  570000|\n",
            "|     Michael Dogbe|Arizona Cardinals|      DE|1996-05-05|  495000|\n",
            "|  Deionte Thompson|Arizona Cardinals|      SS|1997-02-11|  495000|\n",
            "|      Kyler Murray|Arizona Cardinals|      QB|1997-08-07|  495000|\n",
            "|      Joshua Miles|Arizona Cardinals|      OT|1996-01-04|  495000|\n",
            "|      Byron Murphy|Arizona Cardinals|      CB|1998-01-18|  495000|\n",
            "|   Keesean Johnson|Arizona Cardinals|      WR|1996-10-09|  495000|\n",
            "|     Andy Isabella|Arizona Cardinals|      WR|1996-11-18|  495000|\n",
            "|    Jalen Thompson|Arizona Cardinals|      SS|1998-07-18|  495000|\n",
            "+------------------+-----------------+--------+----------+--------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 5.7.\n",
        "\n"
      ],
      "metadata": {
        "id": "_gpnHuqA78e8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    nfl.filter(col(\"Team\") == \"Kansas City Chiefs\")\n",
        "    .orderBy(col(\"Birthday\"))\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1Fijkbk7y6b",
        "outputId": "0aac8a0f-3a2e-4d3e-90a6-d20018a0e37b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+--------+----------+--------+\n",
            "|                Name|              Team|Position|  Birthday|  Salary|\n",
            "+--------------------+------------------+--------+----------+--------+\n",
            "|     Dustin Colquitt|Kansas City Chiefs|       P|1982-05-06| 2450000|\n",
            "|       Terrell Suggs|Kansas City Chiefs|      DE|1982-10-11| 3000000|\n",
            "|    James Winchester|Kansas City Chiefs|      LS|1984-03-10|  820000|\n",
            "|        LeSean McCoy|Kansas City Chiefs|      RB|1988-07-12| 6175000|\n",
            "|     Anthony Sherman|Kansas City Chiefs|      FB|1988-12-11|  930000|\n",
            "|   Stefen Wisniewski|Kansas City Chiefs|       G|1989-03-22| 1250000|\n",
            "|   Mitchell Schwartz|Kansas City Chiefs|      OT|1989-06-08| 6000000|\n",
            "|        Travis Kelce|Kansas City Chiefs|      TE|1989-10-05| 7500000|\n",
            "|    Morris Claiborne|Kansas City Chiefs|      CB|1990-02-07| 1080000|\n",
            "|     Daniel Sorensen|Kansas City Chiefs|      FS|1990-03-05| 3600000|\n",
            "|         Eric Fisher|Kansas City Chiefs|      OT|1991-01-05|10350000|\n",
            "|         Alex Okafor|Kansas City Chiefs|      DE|1991-02-08|  805000|\n",
            "|Laurent Duvernay-...|Kansas City Chiefs|       G|1991-02-11| 6203000|\n",
            "|         Mike Pennel|Kansas City Chiefs|      DT|1991-05-09| 1000000|\n",
            "|          Blake Bell|Kansas City Chiefs|      TE|1991-08-07|  805000|\n",
            "|        Spencer Ware|Kansas City Chiefs|      RB|1991-11-23|  805000|\n",
            "|       Austin Reiter|Kansas City Chiefs|       C|1991-11-27|  975000|\n",
            "|     Xavier Williams|Kansas City Chiefs|      DT|1992-01-18| 2257000|\n",
            "|    Bashaud Breeland|Kansas City Chiefs|      CB|1992-01-30|  805000|\n",
            "|     Damien Williams|Kansas City Chiefs|      RB|1992-04-03| 1050000|\n",
            "+--------------------+------------------+--------+----------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 5.8"
      ],
      "metadata": {
        "id": "DaO5iRYO8eJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    df.selectExpr(\n",
        "        \"median(Salary) as Salary_median\"\n",
        "    )\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGi_rMOD8f1g",
        "outputId": "3f94efa9-0ec3-47fc-a982-6084289a5cf9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+\n",
            "|Salary_median|\n",
            "+-------------+\n",
            "|    3303074.5|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 6.1\n"
      ],
      "metadata": {
        "id": "QxCO8d8o8-F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from pyspark.sql import SparkSession\n",
        "df = pd.read_csv(\"https://bcdanl.github.io/data/netflix.csv\")\n",
        "df = df.where(pd.notnull(df), None)\n",
        "netflix = spark.createDataFrame(df)"
      ],
      "metadata": {
        "id": "lLPdh0ZX8_lK"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1TJ4HjU9Ctc",
        "outputId": "8c57dbd5-938d-458e-c583-943229519dcb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------------+----------------------+----------+-------+\n",
            "|title                             |director              |date_added|type   |\n",
            "+----------------------------------+----------------------+----------+-------+\n",
            "|Alias Grace                       |NULL                  |3-Nov-17  |TV Show|\n",
            "|A Patch of Fog                    |Michael Lennox        |15-Apr-17 |Movie  |\n",
            "|Lunatics                          |NULL                  |19-Apr-19 |TV Show|\n",
            "|Uriyadi 2                         |Vijay Kumar           |2-Aug-19  |Movie  |\n",
            "|Shrek the Musical                 |Jason Moore           |29-Dec-13 |Movie  |\n",
            "|Schubert In Love                  |Lars BÃ¼chel           |1-Mar-18  |Movie  |\n",
            "|We Have Always Lived in the Castle|Stacie Passon         |14-Sep-19 |Movie  |\n",
            "|7SEEDS                            |NULL                  |28-Jun-19 |TV Show|\n",
            "|Voyeur                            |Myles Kane            |1-Dec-17  |Movie  |\n",
            "|You                               |Lee Toland Krieger    |26-Dec-18 |TV Show|\n",
            "|Hasta los dientes                 |Alberto Arnaut Estrada|13-Aug-19 |Movie  |\n",
            "|LEGO House - Home of the Brick    |Anders Falck          |15-Jun-18 |Movie  |\n",
            "|Without Gorky                     |Cosima Spender        |31-May-17 |Movie  |\n",
            "|Chalay Thay Saath                 |Umer Adil             |15-May-18 |Movie  |\n",
            "|Ocean's Thirteen                  |Steven Soderbergh     |1-Oct-19  |Movie  |\n",
            "|Loaded                            |NULL                  |1-Apr-18  |TV Show|\n",
            "|The Family Court                  |NULL                  |1-Nov-17  |TV Show|\n",
            "|Lincoln                           |Steven Spielberg      |21-Feb-18 |Movie  |\n",
            "|The Rugrats Movie                 |Igor Kovalyov         |1-Oct-19  |Movie  |\n",
            "|DJ Cinderella                     |Bruno Garotti         |14-Jun-19 |Movie  |\n",
            "+----------------------------------+----------------------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztGTWXtw9HwL",
        "outputId": "4288fa02-8c16-4338-d481-2bd74ebbe0f6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- director: string (nullable = true)\n",
            " |-- date_added: string (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=EXCEPTION\")\n",
        "# spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"EXCEPTION\")\n",
        "\n",
        "netflix = netflix.withColumn(\"date_added_new\",\n",
        "                             to_date(\"date_added\", \"dd-MMM-yy\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "gRQ7GL409cn2"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(netflix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "DGtGSt4M92mi",
        "outputId": "9a45b1dc-86e2-4497-c593-0ed08030d99d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "itFoFt2P935D",
        "outputId": "73a7fe79-ae74-4d8c-aa74-0e217dd531a6"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o970.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 42) (b415b3b25ae3 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '3-Nov-17' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '3-Nov-17' could not be parsed at index 0\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '3-Nov-17' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '3-Nov-17' could not be parsed at index 0\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 20 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-910a405caf00>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnetflix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o970.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 42) (b415b3b25ae3 executor driver): org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '3-Nov-17' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.time.format.DateTimeParseException: Text '3-Nov-17' could not be parsed at index 0\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 20 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat jdk.internal.reflect.GeneratedMethodAccessor60.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkUpgradeException: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.PARSE_DATETIME_BY_NEW_PARSER] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to parse '3-Nov-17' in the new parser. You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0, or set to \"CORRECTED\" and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError(ExecutionErrors.scala:54)\n\tat org.apache.spark.sql.errors.ExecutionErrors.failToParseDateTimeInNewParserError$(ExecutionErrors.scala:48)\n\tat org.apache.spark.sql.errors.ExecutionErrors$.failToParseDateTimeInNewParserError(ExecutionErrors.scala:218)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:142)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:135)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:195)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text '3-Nov-17' could not be parsed at index 0\n\tat java.base/java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:2046)\n\tat java.base/java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1874)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:193)\n\t... 20 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df_pd = pd.read_csv('https://bcdanl.github.io/data/netflix.csv')\n"
      ],
      "metadata": {
        "id": "fwqrmt_RBpaJ"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pd = df_pd.where(pd.notnull(df_pd), None)\n",
        "netflix = spark.createDataFrame(df_pd)\n"
      ],
      "metadata": {
        "id": "bcHjadxVBq3y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fyNrkXKrBtBq",
        "outputId": "ab551b61-65be-4c96-f2c7-427bd78ae748"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------+-------+\n",
            "|               title|            director|date_added|   type|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "|         Alias Grace|                NULL|  3-Nov-17|TV Show|\n",
            "|      A Patch of Fog|      Michael Lennox| 15-Apr-17|  Movie|\n",
            "|            Lunatics|                NULL| 19-Apr-19|TV Show|\n",
            "|           Uriyadi 2|         Vijay Kumar|  2-Aug-19|  Movie|\n",
            "|   Shrek the Musical|         Jason Moore| 29-Dec-13|  Movie|\n",
            "|    Schubert In Love|         Lars BÃ¼chel|  1-Mar-18|  Movie|\n",
            "|We Have Always Li...|       Stacie Passon| 14-Sep-19|  Movie|\n",
            "|              7SEEDS|                NULL| 28-Jun-19|TV Show|\n",
            "|              Voyeur|          Myles Kane|  1-Dec-17|  Movie|\n",
            "|                 You|  Lee Toland Krieger| 26-Dec-18|TV Show|\n",
            "|   Hasta los dientes|Alberto Arnaut Es...| 13-Aug-19|  Movie|\n",
            "|LEGO House - Home...|        Anders Falck| 15-Jun-18|  Movie|\n",
            "|       Without Gorky|      Cosima Spender| 31-May-17|  Movie|\n",
            "|   Chalay Thay Saath|           Umer Adil| 15-May-18|  Movie|\n",
            "|    Ocean's Thirteen|   Steven Soderbergh|  1-Oct-19|  Movie|\n",
            "|              Loaded|                NULL|  1-Apr-18|TV Show|\n",
            "|    The Family Court|                NULL|  1-Nov-17|TV Show|\n",
            "|             Lincoln|    Steven Spielberg| 21-Feb-18|  Movie|\n",
            "|   The Rugrats Movie|       Igor Kovalyov|  1-Oct-19|  Movie|\n",
            "|       DJ Cinderella|       Bruno Garotti| 14-Jun-19|  Movie|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to date or timestamp\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "# Casting the \"Birthday\" column to a date type\n",
        "netflix = netflix.withColumn(\"date_added\", to_date(\"date_added\", \"d-MMM-yy\"))\n",
        "netflix.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9uO3qUTBuwe",
        "outputId": "1e148261-c952-4579-b81d-2455b57d3287"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------+-------+\n",
            "|               title|            director|date_added|   type|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "|         Alias Grace|                NULL|2017-11-03|TV Show|\n",
            "|      A Patch of Fog|      Michael Lennox|2017-04-15|  Movie|\n",
            "|            Lunatics|                NULL|2019-04-19|TV Show|\n",
            "|           Uriyadi 2|         Vijay Kumar|2019-08-02|  Movie|\n",
            "|   Shrek the Musical|         Jason Moore|2013-12-29|  Movie|\n",
            "|    Schubert In Love|         Lars BÃ¼chel|2018-03-01|  Movie|\n",
            "|We Have Always Li...|       Stacie Passon|2019-09-14|  Movie|\n",
            "|              7SEEDS|                NULL|2019-06-28|TV Show|\n",
            "|              Voyeur|          Myles Kane|2017-12-01|  Movie|\n",
            "|                 You|  Lee Toland Krieger|2018-12-26|TV Show|\n",
            "|   Hasta los dientes|Alberto Arnaut Es...|2019-08-13|  Movie|\n",
            "|LEGO House - Home...|        Anders Falck|2018-06-15|  Movie|\n",
            "|       Without Gorky|      Cosima Spender|2017-05-31|  Movie|\n",
            "|   Chalay Thay Saath|           Umer Adil|2018-05-15|  Movie|\n",
            "|    Ocean's Thirteen|   Steven Soderbergh|2019-10-01|  Movie|\n",
            "|              Loaded|                NULL|2018-04-01|TV Show|\n",
            "|    The Family Court|                NULL|2017-11-01|TV Show|\n",
            "|             Lincoln|    Steven Spielberg|2018-02-21|  Movie|\n",
            "|   The Rugrats Movie|       Igor Kovalyov|2019-10-01|  Movie|\n",
            "|       DJ Cinderella|       Bruno Garotti|2019-06-14|  Movie|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filter"
      ],
      "metadata": {
        "id": "ZE0olVXTCEZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ad_3JHRgCEHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "df_pd = pd.read_csv(\"https://bcdanl.github.io/data/employment.csv\")\n",
        "df_pd = df_pd.where(pd.notnull(df_pd), None)  # Convert NaN to None\n",
        "df = spark.createDataFrame(df_pd)\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWyAkVIFBwca",
        "outputId": "3b37005d-9851-4e32-8c15-cfacc3b230ac"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+------------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|        Team|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "|   Douglas|  Male|    8/6/93|     NaN| true|   Marketing|\n",
            "|    Thomas|  Male|   3/31/96| 61933.0| true|        NULL|\n",
            "|     Maria|Female|      NULL|130590.0|false|     Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|     Finance|\n",
            "|     Larry|  Male|   1/24/98|101004.0| true|          IT|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|       Legal|\n",
            "|      Ruby|Female|   8/17/87| 65476.0| true|     Product|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|     Finance|\n",
            "|    Angela|Female|  11/22/05| 95570.0| true| Engineering|\n",
            "|   Frances|Female|    8/8/02|139852.0| true|Business Dev|\n",
            "|    Louise|Female|   8/12/80| 63241.0| true|        NULL|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|       Legal|\n",
            "|   Brandon|  Male|   12/1/80|112807.0| true|          HR|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|       Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|     Finance|\n",
            "|   Lillian|Female|    6/5/16| 59414.0|false|     Product|\n",
            "|    Jeremy|  Male|   9/21/10| 90370.0|false|          HR|\n",
            "|     Shawn|  Male|   12/7/86|111737.0|false|     Product|\n",
            "|     Diana|Female|  10/23/81|132940.0|false|          IT|\n",
            "|     Donna|Female|   7/22/10| 81014.0|false|     Product|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avVZbrBlBySO",
        "outputId": "dfaec76a-6604-4f65-d128-e03145c75004"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- First Name: string (nullable = true)\n",
            " |-- Gender: string (nullable = true)\n",
            " |-- Start Date: string (nullable = true)\n",
            " |-- Salary: double (nullable = true)\n",
            " |-- Mgmt: boolean (nullable = true)\n",
            " |-- Team: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"Salary\") > 100000).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6G4MItHBzkO",
        "outputId": "ec8366a8-68c6-4736-c11b-f65c9f2b08a5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+------------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|        Team|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "|   Douglas|  Male|    8/6/93|     NaN| true|   Marketing|\n",
            "|     Maria|Female|      NULL|130590.0|false|     Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|     Finance|\n",
            "|     Larry|  Male|   1/24/98|101004.0| true|          IT|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|       Legal|\n",
            "|   Frances|Female|    8/8/02|139852.0| true|Business Dev|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|       Legal|\n",
            "|   Brandon|  Male|   12/1/80|112807.0| true|          HR|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|       Sales|\n",
            "|     Shawn|  Male|   12/7/86|111737.0|false|     Product|\n",
            "|     Diana|Female|  10/23/81|132940.0|false|          IT|\n",
            "|   Matthew|  Male|    9/5/95|100612.0|false|   Marketing|\n",
            "|      NULL|  Male|   6/14/12|125792.0| NULL|        NULL|\n",
            "|     Scott|  NULL|   7/11/91|122367.0|false|       Legal|\n",
            "|     Terry|  Male|  11/27/81|124008.0| true|          IT|\n",
            "| Christina|Female|    8/6/02|118780.0| true| Engineering|\n",
            "|      NULL|  Male|   8/21/98|122340.0| NULL|        NULL|\n",
            "|      Jean|Female|  12/18/93|119082.0|false|Business Dev|\n",
            "|    Rachel|Female|   2/16/09|142032.0|false|Business Dev|\n",
            "|      NULL|  Male|   1/29/16|122173.0| NULL|          IT|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    df.filter(\n",
        "        (col(\"Team\") == \"Finance\") & (col(\"Salary\") > 100000)\n",
        "    ).show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3tjtkztB1E0",
        "outputId": "e003aaeb-f3c3-42ed-b80d-651972677b72"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+-------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|   Team|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "|     Maria|Female|      NULL|130590.0|false|Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|Finance|\n",
            "|     Bruce|  Male|  11/28/09|114796.0|false|Finance|\n",
            "|      Carl|  Male|    5/3/06|130276.0| true|Finance|\n",
            "|     Irene|  NULL|   7/14/15|100863.0| true|Finance|\n",
            "|     Shawn|  Male|   9/23/05|148115.0| true|Finance|\n",
            "|   Cynthia|Female|   3/21/94|142321.0|false|Finance|\n",
            "|   Phyllis|Female|  10/11/96|136984.0| true|Finance|\n",
            "|    Steven|  Male|    3/1/95|109095.0|false|Finance|\n",
            "| Elizabeth|Female|   10/9/03|146129.0|false|Finance|\n",
            "|     Kathy|Female|   3/18/00|149563.0| true|Finance|\n",
            "|   Brandon|  Male|   3/27/06|115711.0| true|Finance|\n",
            "|     Billy|  Male|   3/13/95|120444.0| true|Finance|\n",
            "|    Walter|  Male|    7/6/83|127813.0|false|Finance|\n",
            "|       Roy|  Male|    8/6/06|148225.0|false|Finance|\n",
            "|    Philip|  Male|    8/2/89|129968.0|false|Finance|\n",
            "|    Justin|  Male|   5/22/86|121508.0| true|Finance|\n",
            "|    Albert|  Male|   9/30/07|102626.0|false|Finance|\n",
            "|    Ashley|Female|   8/15/97|142415.0| true|Finance|\n",
            "|   Barbara|Female|   3/22/04|144677.0|false|Finance|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\n",
        "    (col(\"Team\") == \"Finance\") |\n",
        "    (col(\"Team\") == \"Legal\")   |\n",
        "    (col(\"Team\") == \"Sales\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvYIXSSrB2ni",
        "outputId": "ba8c1fcd-b508-448b-ac25-033c6ffbf53c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+-------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|   Team|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "|     Maria|Female|      NULL|130590.0|false|Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|Finance|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|  Legal|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|Finance|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|  Legal|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|  Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|Finance|\n",
            "|      Lois|  NULL|   4/22/95| 64714.0| true|  Legal|\n",
            "|     Scott|  NULL|   7/11/91|122367.0|false|  Legal|\n",
            "|  Benjamin|  Male|   1/26/05| 79529.0| true|  Legal|\n",
            "|   Theresa|Female|  10/10/06| 85182.0|false|  Sales|\n",
            "|   Beverly|Female|    9/9/98|121918.0|false|  Legal|\n",
            "|     Roger|  Male|   4/17/80| 88010.0| true|  Sales|\n",
            "|     Bruce|  Male|  11/28/09|114796.0|false|Finance|\n",
            "|     Chris|  NULL|   1/24/80|113590.0|false|  Sales|\n",
            "|      NULL|  NULL|  12/17/11| 41126.0| NULL|  Sales|\n",
            "|      Alan|  NULL|    3/3/14| 40341.0| true|Finance|\n",
            "|      Carl|  Male|    5/3/06|130276.0| true|Finance|\n",
            "|    Rachel|Female|   8/16/99| 51178.0| true|Finance|\n",
            "|      Jose|  Male|  10/30/04| 84834.0| true|Finance|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    df.filter(\n",
        "    (col(\"Team\") == \"Finance\") |\n",
        "    (col(\"Team\") == \"Legal\")   |\n",
        "    (col(\"Team\") == \"Sales\"))\n",
        "    .groupBy(\"Team\")\n",
        "    .count()\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6D_rijvB4RI",
        "outputId": "842f69ab-e4a3-4390-a34e-fe45d455b5c7"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-----+\n",
            "|   Team|count|\n",
            "+-------+-----+\n",
            "|  Sales|   94|\n",
            "|Finance|  102|\n",
            "|  Legal|   88|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(col(\"Team\").isin(\"Finance\", \"Legal\", \"Sales\")).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjWa_sD5B5-h",
        "outputId": "d1034f54-7e1e-4727-d65d-5e22907e4337"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+-------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|   Team|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "|     Maria|Female|      NULL|130590.0|false|Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|Finance|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|  Legal|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|Finance|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|  Legal|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|  Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|Finance|\n",
            "|      Lois|  NULL|   4/22/95| 64714.0| true|  Legal|\n",
            "|     Scott|  NULL|   7/11/91|122367.0|false|  Legal|\n",
            "|  Benjamin|  Male|   1/26/05| 79529.0| true|  Legal|\n",
            "|   Theresa|Female|  10/10/06| 85182.0|false|  Sales|\n",
            "|   Beverly|Female|    9/9/98|121918.0|false|  Legal|\n",
            "|     Roger|  Male|   4/17/80| 88010.0| true|  Sales|\n",
            "|     Bruce|  Male|  11/28/09|114796.0|false|Finance|\n",
            "|     Chris|  NULL|   1/24/80|113590.0|false|  Sales|\n",
            "|      NULL|  NULL|  12/17/11| 41126.0| NULL|  Sales|\n",
            "|      Alan|  NULL|    3/3/14| 40341.0| true|Finance|\n",
            "|      Carl|  Male|    5/3/06|130276.0| true|Finance|\n",
            "|    Rachel|Female|   8/16/99| 51178.0| true|Finance|\n",
            "|      Jose|  Male|  10/30/04| 84834.0| true|Finance|\n",
            "+----------+------+----------+--------+-----+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(\n",
        "    col(\"Salary\").between(90000,100000)\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knMCprBaB7cJ",
        "outputId": "4b44dda9-98eb-45cb-e870-d8bfef1a768d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+-------+-----+------------+\n",
            "|First Name|Gender|Start Date| Salary| Mgmt|        Team|\n",
            "+----------+------+----------+-------+-----+------------+\n",
            "|    Angela|Female|  11/22/05|95570.0| true| Engineering|\n",
            "|    Jeremy|  Male|   9/21/10|90370.0|false|          HR|\n",
            "|    Joshua|  NULL|    3/8/12|90816.0| true|          IT|\n",
            "|      John|  Male|    7/1/92|97950.0|false|          IT|\n",
            "|     Jerry|  Male|   1/10/04|95734.0|false|          IT|\n",
            "|   Michael|  Male|  10/10/08|99283.0| true|Distribution|\n",
            "|  Clarence|  Male|   3/26/96|93581.0| true|Business Dev|\n",
            "|     Nancy|Female|   9/23/00|94976.0| true| Engineering|\n",
            "|   Frances|Female|    4/4/99|90582.0| true|       Sales|\n",
            "|    Janice|  NULL|   8/21/97|91719.0| true|       Legal|\n",
            "|     Kathy|Female|    3/9/96|91712.0|false|     Finance|\n",
            "|   Rebecca|Female|   7/10/92|94231.0|false|     Product|\n",
            "|      Adam|  Male|   5/21/11|95327.0|false|Distribution|\n",
            "|  Patricia|Female|  11/25/92|95322.0|false|     Product|\n",
            "|      Sara|Female|   9/23/91|97058.0|false|     Finance|\n",
            "|      Ruth|  NULL|   5/18/99|98233.0| true|Distribution|\n",
            "|   Barbara|  NULL|    5/3/03|99326.0| true|       Legal|\n",
            "|      Mary|Female|   5/30/09|92544.0|false|          IT|\n",
            "|   Gregory|  NULL|   6/15/92|98865.0| true|     Finance|\n",
            "|     Norma|Female|   2/11/15|94393.0| true| Engineering|\n",
            "+----------+------+----------+-------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 6"
      ],
      "metadata": {
        "id": "SNHWaqSkB92U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "(\n",
        "    netflix\n",
        "    .filter(netflix.director == \"Martin Scorsese\")\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450jS5R0B9C3",
        "outputId": "d0893829-b4a5-4821-b3c3-885ab9eccdc9"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+---------------+----------+-----+\n",
            "|               title|       director|date_added| type|\n",
            "+--------------------+---------------+----------+-----+\n",
            "|         Raging Bull|Martin Scorsese|2019-10-01|Movie|\n",
            "|   Gangs of New York|Martin Scorsese|2019-08-20|Movie|\n",
            "|        Mean Streets|Martin Scorsese|2019-07-01|Movie|\n",
            "|         Taxi Driver|Martin Scorsese|2019-07-01|Movie|\n",
            "|Rolling Thunder R...|Martin Scorsese|2019-06-12|Movie|\n",
            "|Who's That Knocki...|Martin Scorsese|2019-07-01|Movie|\n",
            "|Alice Doesn't Liv...|Martin Scorsese|2019-07-01|Movie|\n",
            "|        The Irishman|Martin Scorsese|2019-11-27|Movie|\n",
            "+--------------------+---------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "(\n",
        "    netflix\n",
        "    .filter((netflix.title == \"Limitless\") &\n",
        "            (netflix.type == \"Movie\"))\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l51WVJBHCX6C",
        "outputId": "ff50fba9-e645-4abe-bb69-b0e94299f52f"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+----------+-----+\n",
            "|    title|       director|date_added| type|\n",
            "+---------+---------------+----------+-----+\n",
            "|Limitless|    Neil Burger|2019-05-16|Movie|\n",
            "|Limitless|Vrinda Samartha|2019-10-01|Movie|\n",
            "+---------+---------------+----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "(\n",
        "    netflix\n",
        "    .filter((netflix.director == \"Bong Joon Ho\") |\n",
        "            (netflix.date_added == \"2018-06-15\"))\n",
        "    .show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXYPh4ghCmGi",
        "outputId": "8075ed1c-549a-40ce-f22a-20067cd5f0f8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+----------+-------+\n",
            "|               title|            director|date_added|   type|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "|LEGO House - Home...|        Anders Falck|2018-06-15|  Movie|\n",
            "|        Lust Stories|         Zoya Akhtar|2018-06-15|  Movie|\n",
            "|Pacificum: Return...|     Mariana Tschudi|2018-06-15|  Movie|\n",
            "|           Set It Up|      Claire Scanlon|2018-06-15|  Movie|\n",
            "|True: Wonderful W...|                NULL|2018-06-15|TV Show|\n",
            "|       The last hour|Eduardo Mendoza d...|2018-06-15|  Movie|\n",
            "|True: Magical Fri...|                NULL|2018-06-15|TV Show|\n",
            "|    Sunday's Illness|       RamÃ³n Salazar|2018-06-15|  Movie|\n",
            "|              Maktub|            Oded Raz|2018-06-15|  Movie|\n",
            "|Chronicle of an E...|Israel AdriÃ¡n Cae...|2018-06-15|  Movie|\n",
            "|                Okja|        Bong Joon Ho|2017-06-28|  Movie|\n",
            "|         Snowpiercer|        Bong Joon Ho|2019-05-01|  Movie|\n",
            "+--------------------+--------------------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5\n",
        "# Find all observations with a director of âEthan Coenâ, âJoel Coenâ, and âQuentin Tarantinoâ.\n",
        "\n",
        "(\n",
        "    netflix\n",
        "    .filter(\n",
        "        netflix.director.isin(\"Ethan Coen\", \"Joel Coen\", \"Quentin Tarantino\")\n",
        "    ).show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQ-gY1BAC4EP",
        "outputId": "51cf210a-d856-4854-839f-3ed28d19e290"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------------+----------+-------+\n",
            "|               title|         director|date_added|   type|\n",
            "+--------------------+-----------------+----------+-------+\n",
            "|       A Serious Man|       Ethan Coen|2018-01-16|  Movie|\n",
            "|Inglourious Basterds|Quentin Tarantino|2019-07-22|  Movie|\n",
            "|The Hateful Eight...|Quentin Tarantino|2019-04-25|TV Show|\n",
            "|The Ballad of Bus...|        Joel Coen|2018-11-16|  Movie|\n",
            "|   The Hateful Eight|Quentin Tarantino|2017-10-25|  Movie|\n",
            "|        Pulp Fiction|Quentin Tarantino|2019-01-01|  Movie|\n",
            "|        Jackie Brown|Quentin Tarantino|2019-08-01|  Movie|\n",
            "+--------------------+-----------------+----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q6\n",
        "# Find all observations with a date_added value between January 1, 2019 and February 1, 2019.\n",
        "\n",
        "(\n",
        "    netflix\n",
        "    .filter(\n",
        "        netflix.date_added.between(\"2019-01-01\", \"2019-02-01\")\n",
        "    ).show()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfsx36trDMvX",
        "outputId": "fdff25b2-caf5-456c-a4c5-6a1d9b456c57"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+----------+-------+\n",
            "|               title|           director|date_added|   type|\n",
            "+--------------------+-------------------+----------+-------+\n",
            "|          Good Girls|               NULL|2019-01-01|TV Show|\n",
            "|Sebastian Manisca...|   Rik Reinholdtsen|2019-01-15|  Movie|\n",
            "|       The Big Catch|               NULL|2019-02-01|TV Show|\n",
            "|           Malicious|    Michael Winnick|2019-02-01|  Movie|\n",
            "|         About a Boy|        Chris Weitz|2019-02-01|  Movie|\n",
            "|        Echcharikkai|             Sarjun|2019-01-15|  Movie|\n",
            "|             Justice|Ahmed Khaled Moussa|2019-01-21|TV Show|\n",
            "|        Rudy Habibie|   Hanung Bramantyo|2019-01-28|  Movie|\n",
            "|         The Monster|      Bryan Bertino|2019-01-09|  Movie|\n",
            "| My Tattoo Addiction|               NULL|2019-02-01|TV Show|\n",
            "|            The Pass|    Ben A. Williams|2019-01-01|  Movie|\n",
            "| Up Among  The Stars|      Zoe BerriatÃºa|2019-01-29|  Movie|\n",
            "|The Boy in the St...|        Mark Herman|2019-01-01|  Movie|\n",
            "|LEGO Ninjago: Mas...|               NULL|2019-02-01|  Movie|\n",
            "|       Komola Rocket|   Noor Imran Mithu|2019-01-07|  Movie|\n",
            "|Examination of Co...|               NULL|2019-01-25|TV Show|\n",
            "|      5 Cowok Jagoan|       Anggy Umbara|2019-01-05|  Movie|\n",
            "|Abducted in Plain...|       Skye Borgman|2019-01-15|  Movie|\n",
            "|            Innocent|         Seren YÃ¼ce|2019-01-23|TV Show|\n",
            "|Catwalk: Tales fr...|       Aaron Hancox|2019-01-07|  Movie|\n",
            "+--------------------+-------------------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# missing values"
      ],
      "metadata": {
        "id": "V1uje4pdDmmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_notNA = df.na.drop(subset = [\"Gender\", \"Team\"])"
      ],
      "metadata": {
        "id": "7P6GuUMMDocT"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_notNA.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Rn0WL2IDzl4",
        "outputId": "de715650-9cc3-48d8-8582-1ead9678a41d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "815"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa2nlXs6D1h1",
        "outputId": "577438ff-17d0-4a19-92e3-35aaa82f2c7b"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1001"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfu3fQVEEOEe",
        "outputId": "4b8f3603-15cf-4223-976f-8d72ff96fe00"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+------------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|        Team|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "|   Douglas|  Male|    8/6/93|     NaN| true|   Marketing|\n",
            "|    Thomas|  Male|   3/31/96| 61933.0| true|        NULL|\n",
            "|     Maria|Female|      NULL|130590.0|false|     Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|     Finance|\n",
            "|     Larry|  Male|   1/24/98|101004.0| true|          IT|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|       Legal|\n",
            "|      Ruby|Female|   8/17/87| 65476.0| true|     Product|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|     Finance|\n",
            "|    Angela|Female|  11/22/05| 95570.0| true| Engineering|\n",
            "|   Frances|Female|    8/8/02|139852.0| true|Business Dev|\n",
            "|    Louise|Female|   8/12/80| 63241.0| true|        NULL|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|       Legal|\n",
            "|   Brandon|  Male|   12/1/80|112807.0| true|          HR|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|       Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|     Finance|\n",
            "|   Lillian|Female|    6/5/16| 59414.0|false|     Product|\n",
            "|    Jeremy|  Male|   9/21/10| 90370.0|false|          HR|\n",
            "|     Shawn|  Male|   12/7/86|111737.0|false|     Product|\n",
            "|     Diana|Female|  10/23/81|132940.0|false|          IT|\n",
            "|     Donna|Female|   7/22/10| 81014.0|false|     Product|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.na.fill(\n",
        "    value = 0,\n",
        "    subset = [\"Salary\"]\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_YAXAuDEIO6",
        "outputId": "f2b4e17f-23e6-48b4-d8e5-6fc6fc80812e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+------------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|        Team|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "|   Douglas|  Male|    8/6/93|     0.0| true|   Marketing|\n",
            "|    Thomas|  Male|   3/31/96| 61933.0| true|        NULL|\n",
            "|     Maria|Female|      NULL|130590.0|false|     Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|     Finance|\n",
            "|     Larry|  Male|   1/24/98|101004.0| true|          IT|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|       Legal|\n",
            "|      Ruby|Female|   8/17/87| 65476.0| true|     Product|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|     Finance|\n",
            "|    Angela|Female|  11/22/05| 95570.0| true| Engineering|\n",
            "|   Frances|Female|    8/8/02|139852.0| true|Business Dev|\n",
            "|    Louise|Female|   8/12/80| 63241.0| true|        NULL|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|       Legal|\n",
            "|   Brandon|  Male|   12/1/80|112807.0| true|          HR|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|       Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|     Finance|\n",
            "|   Lillian|Female|    6/5/16| 59414.0|false|     Product|\n",
            "|    Jeremy|  Male|   9/21/10| 90370.0|false|          HR|\n",
            "|     Shawn|  Male|   12/7/86|111737.0|false|     Product|\n",
            "|     Diana|Female|  10/23/81|132940.0|false|          IT|\n",
            "|     Donna|Female|   7/22/10| 81014.0|false|     Product|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.na.fill(\n",
        "    {\"Salary\": 0,\n",
        "     \"Team\": \"Unknown\"}\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u47Xya5wEZw9",
        "outputId": "85308022-57ae-4636-e6c8-c884526d8ad9"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+--------+-----+------------+\n",
            "|First Name|Gender|Start Date|  Salary| Mgmt|        Team|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "|   Douglas|  Male|    8/6/93|     0.0| true|   Marketing|\n",
            "|    Thomas|  Male|   3/31/96| 61933.0| true|     Unknown|\n",
            "|     Maria|Female|      NULL|130590.0|false|     Finance|\n",
            "|     Jerry|  NULL|    3/4/05|138705.0| true|     Finance|\n",
            "|     Larry|  Male|   1/24/98|101004.0| true|          IT|\n",
            "|    Dennis|  Male|   4/18/87|115163.0|false|       Legal|\n",
            "|      Ruby|Female|   8/17/87| 65476.0| true|     Product|\n",
            "|      NULL|Female|   7/20/15| 45906.0| NULL|     Finance|\n",
            "|    Angela|Female|  11/22/05| 95570.0| true| Engineering|\n",
            "|   Frances|Female|    8/8/02|139852.0| true|Business Dev|\n",
            "|    Louise|Female|   8/12/80| 63241.0| true|     Unknown|\n",
            "|     Julie|Female|  10/26/97|102508.0| true|       Legal|\n",
            "|   Brandon|  Male|   12/1/80|112807.0| true|          HR|\n",
            "|      Gary|  Male|   1/27/08|109831.0|false|       Sales|\n",
            "|  Kimberly|Female|   1/14/99| 41426.0| true|     Finance|\n",
            "|   Lillian|Female|    6/5/16| 59414.0|false|     Product|\n",
            "|    Jeremy|  Male|   9/21/10| 90370.0|false|          HR|\n",
            "|     Shawn|  Male|   12/7/86|111737.0|false|     Product|\n",
            "|     Diana|Female|  10/23/81|132940.0|false|          IT|\n",
            "|     Donna|Female|   7/22/10| 81014.0|false|     Product|\n",
            "+----------+------+----------+--------+-----+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 6.7"
      ],
      "metadata": {
        "id": "MLt83I7jEuab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    netflix\n",
        "    .na.drop(subset = ['director'])\n",
        "    .count()\n",
        "\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBW_r1q2Etx0",
        "outputId": "2835ad31-49f0-41fd-c765-3f479369ada9"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3936"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnajDAWiE_Hp",
        "outputId": "d8906840-0b23-4a7e-9a8c-57372e7c127d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5837"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CW 6.8"
      ],
      "metadata": {
        "id": "n1edsmAvFE1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify the days when Netflix added only one movie to its catalog.\n",
        "\n",
        "netflix.groupBy(\"date_added\").count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UoMUcLdFEff",
        "outputId": "2cfd85b4-766c-448d-cd41-f50f34b4a50f"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|date_added|count|\n",
            "+----------+-----+\n",
            "|2019-05-08|    9|\n",
            "|2018-08-10|    8|\n",
            "|2016-03-01|    5|\n",
            "|2014-09-26|    1|\n",
            "|2017-09-11|    1|\n",
            "|2019-06-04|    4|\n",
            "|2019-11-01|   94|\n",
            "|2017-01-06|    4|\n",
            "|2015-03-06|    2|\n",
            "|2019-09-22|    2|\n",
            "|2018-08-08|    2|\n",
            "|2018-10-05|    9|\n",
            "|2018-09-01|   23|\n",
            "|2018-11-02|    7|\n",
            "|2016-08-15|    5|\n",
            "|2015-09-02|    1|\n",
            "|2018-06-26|    3|\n",
            "|2017-02-26|    1|\n",
            "|2019-05-27|    3|\n",
            "|2017-09-28|    1|\n",
            "+----------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(netflix.groupBy(\"date_added\").count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "FVmxOU-UFkSj",
        "outputId": "9522a1f0-413e-469f-d026-4f2a6039cfd8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "netflix.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_k-45FFF9Vk",
        "outputId": "ea4811e8-5dc2-44ff-9f5c-fac3768242d3"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- director: string (nullable = true)\n",
            " |-- date_added: date (nullable = true)\n",
            " |-- type: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(\n",
        "    netflix\n",
        "    .filter(col(\"type\") == \"Movie\")\n",
        "    .groupBy(\"date_added\")\n",
        "    .count()\n",
        "    .filter(col(\"count\") == 1)\n",
        "    .count()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_Fbl68pFm5k",
        "outputId": "35bc43ff-21d6-4981-931f-a95ea3476179"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "426"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Group Operations"
      ],
      "metadata": {
        "id": "CBaMhrysGXCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "food_data = [\n",
        "    (\"Apple\", \"Fruit\", 1.05),\n",
        "    (\"Onion\", \"Vegie\", 1.00),\n",
        "    (\"Orange\", \"Fruit\", 1.25),\n",
        "    (\"Tomato\", \"Vegie\", 0.85),\n",
        "    (\"Watermelon\", \"Fruit\", 4.15)\n",
        "]\n",
        "\n",
        "food_df = spark.createDataFrame(food_data, [\"Item\", \"Type\", \"Price\"])\n",
        "\n",
        "# Group by \"Type\"\n",
        "groups = food_df.groupBy(\"Type\")"
      ],
      "metadata": {
        "id": "odXuTV7WGYba"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDszFxIhGZn6",
        "outputId": "11875a07-2484-4188-928b-f6193c6ca4f9"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+\n",
            "|      Item| Type|Price|\n",
            "+----------+-----+-----+\n",
            "|     Apple|Fruit| 1.05|\n",
            "|     Onion|Vegie|  1.0|\n",
            "|    Orange|Fruit| 1.25|\n",
            "|    Tomato|Vegie| 0.85|\n",
            "|Watermelon|Fruit| 4.15|\n",
            "+----------+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groups.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "M6dUtFG3GcYn",
        "outputId": "6a03b29d-7b2a-4fa9-868c-2f5a45f7ef86"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GroupedData' object has no attribute 'show'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-535c07fb4031>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgroups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'GroupedData' object has no attribute 'show'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(groups)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "dL8gDuJDGfST",
        "outputId": "106daeec-b5bd-457f-b3ee-4b7f426421f3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.group.GroupedData"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.group.GroupedData</b><br/>def __init__(jgd: JavaObject, df: DataFrame)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/sql/group.py</a>A set of methods for aggregations on a :class:`DataFrame`,\n",
              "created by :func:`DataFrame.groupBy`.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 57);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groups.avg(\"Price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB1LiUYZGrus",
        "outputId": "b905346c-ba9d-459b-a828-c89d2a2c3eb1"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| Type|avg(Price)|\n",
            "+-----+----------+\n",
            "|Vegie|     0.925|\n",
            "|Fruit|      2.15|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate the sum of the Price for each Type\n",
        "groups.sum(\"Price\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFoPkbJMGzeT",
        "outputId": "95d7a0a8-dccf-41c1-b2fb-47b52ba99fe5"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+----------+\n",
            "| Type|sum(Price)|\n",
            "+-----+----------+\n",
            "|Vegie|      1.85|\n",
            "|Fruit|      6.45|\n",
            "+-----+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count how many rows in each Type\n",
        "groups.count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rzgtat66G01-",
        "outputId": "5f898e5c-8c21-41a9-a4fc-7f6db3232b77"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "| Type|count|\n",
            "+-----+-----+\n",
            "|Vegie|    2|\n",
            "|Fruit|    3|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import min, max, mean\n",
        "\n",
        "food_df.groupBy(\"Type\").agg(\n",
        "    min(\"Price\").alias(\"min_price\"),\n",
        "    max(\"Price\").alias(\"max_price\"),\n",
        "    mean(\"Price\").alias(\"mean_price\")\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME4kpG1cG9Of",
        "outputId": "779ebc48-1079-457e-e06f-987442e41a87"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+---------+----------+\n",
            "| Type|min_price|max_price|mean_price|\n",
            "+-----+---------+---------+----------+\n",
            "|Vegie|     0.85|      1.0|     0.925|\n",
            "|Fruit|     1.05|     4.15|      2.15|\n",
            "+-----+---------+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import avg, col\n",
        "\n",
        "# Define a window partitioned by \"Type\"\n",
        "w = Window.partitionBy(\"Type\")\n",
        "\n",
        "food_df_with_mean = food_df.withColumn(\n",
        "    \"mean_price_by_type\",\n",
        "    avg(col(\"Price\")).over(w)\n",
        ")\n",
        "food_df_with_mean.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbhf7Lq8HVHm",
        "outputId": "093458ae-36c4-4bde-f5cf-c9df31a94ed1"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+-----+------------------+\n",
            "|      Item| Type|Price|mean_price_by_type|\n",
            "+----------+-----+-----+------------------+\n",
            "|     Apple|Fruit| 1.05|              2.15|\n",
            "|    Orange|Fruit| 1.25|              2.15|\n",
            "|Watermelon|Fruit| 4.15|              2.15|\n",
            "|     Onion|Vegie|  1.0|             0.925|\n",
            "|    Tomato|Vegie| 0.85|             0.925|\n",
            "+----------+-----+-----+------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}